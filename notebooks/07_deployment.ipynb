{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23106a9c",
   "metadata": {},
   "source": [
    "## 1. Setup & Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f86f408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Setup complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Paths\n",
    "MODELS_DIR = Path('../models')\n",
    "PROCESSED_DATA_DIR = Path('../data/processed')\n",
    "SRC_DIR = Path('../src')\n",
    "SRC_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3aaf778d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Model: random_forest\n",
      "üìä Features: 18\n",
      "üéØ Target: fare_amount\n"
     ]
    }
   ],
   "source": [
    "# Load model and configuration\n",
    "model = joblib.load(MODELS_DIR / 'tuned_model.joblib')\n",
    "model_metadata = joblib.load(MODELS_DIR / 'tuned_model_metadata.joblib')\n",
    "feature_config = joblib.load(PROCESSED_DATA_DIR / 'feature_config.joblib')\n",
    "\n",
    "ALL_FEATURES = feature_config['all_features']\n",
    "TARGET = feature_config['target']\n",
    "MODEL_NAME = model_metadata['model_name']\n",
    "\n",
    "print(f\"ü§ñ Model: {MODEL_NAME}\")\n",
    "print(f\"üìä Features: {len(ALL_FEATURES)}\")\n",
    "print(f\"üéØ Target: {TARGET}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9923d0d",
   "metadata": {},
   "source": [
    "## 2. Create Production-Ready Model Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03ce55ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Production model saved: ../models/production_model.joblib\n",
      "   Version: 1.0.0\n",
      "   Created: 2025-12-13 10:01:23\n"
     ]
    }
   ],
   "source": [
    "# Create comprehensive model package for deployment\n",
    "model_package = {\n",
    "    'model': model,\n",
    "    'features': ALL_FEATURES,\n",
    "    'target': TARGET,\n",
    "    'model_name': MODEL_NAME,\n",
    "    'model_type': type(model).__name__,\n",
    "    'version': '1.0.0',\n",
    "    'created_at': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "}\n",
    "\n",
    "# Save production model package\n",
    "production_model_path = MODELS_DIR / 'production_model.joblib'\n",
    "joblib.dump(model_package, production_model_path)\n",
    "\n",
    "print(f\"‚úÖ Production model saved: {production_model_path}\")\n",
    "print(f\"   Version: {model_package['version']}\")\n",
    "print(f\"   Created: {model_package['created_at']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "859444fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Feature config saved as JSON\n"
     ]
    }
   ],
   "source": [
    "# Save feature configuration as JSON (for API validation)\n",
    "feature_info = {\n",
    "    'features': ALL_FEATURES,\n",
    "    'target': TARGET,\n",
    "    'model_name': MODEL_NAME\n",
    "}\n",
    "\n",
    "with open(MODELS_DIR / 'feature_config.json', 'w') as f:\n",
    "    json.dump(feature_info, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Feature config saved as JSON\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add229bf",
   "metadata": {},
   "source": [
    "## 3. Create Inference Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18c30ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Predictor class defined\n"
     ]
    }
   ],
   "source": [
    "class NYCTaxiFarePredictor:\n",
    "    \"\"\"\n",
    "    Production-ready predictor for NYC Taxi Fare.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str):\n",
    "        \"\"\"Load model from path.\"\"\"\n",
    "        package = joblib.load(model_path)\n",
    "        self.model = package['model']\n",
    "        self.features = package['features']\n",
    "        self.target = package['target']\n",
    "        self.model_name = package['model_name']\n",
    "        self.version = package['version']\n",
    "        print(f\"‚úÖ Loaded {self.model_name} v{self.version}\")\n",
    "    \n",
    "    def predict(self, data: dict) -> float:\n",
    "        \"\"\"\n",
    "        Make single prediction from dictionary input.\n",
    "        \n",
    "        Args:\n",
    "            data: Dictionary with feature values\n",
    "            \n",
    "        Returns:\n",
    "            Predicted fare amount\n",
    "        \"\"\"\n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame([data])\n",
    "        \n",
    "        # Ensure all features present\n",
    "        for feature in self.features:\n",
    "            if feature not in df.columns:\n",
    "                raise ValueError(f\"Missing feature: {feature}\")\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = self.model.predict(df[self.features])[0]\n",
    "        return float(prediction)\n",
    "    \n",
    "    def predict_batch(self, data: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Make batch predictions from DataFrame.\n",
    "        \n",
    "        Args:\n",
    "            data: DataFrame with feature columns\n",
    "            \n",
    "        Returns:\n",
    "            Array of predicted fares\n",
    "        \"\"\"\n",
    "        return self.model.predict(data[self.features])\n",
    "    \n",
    "    def get_info(self) -> dict:\n",
    "        \"\"\"Return model information.\"\"\"\n",
    "        return {\n",
    "            'model_name': self.model_name,\n",
    "            'version': self.version,\n",
    "            'features': self.features,\n",
    "            'target': self.target\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ Predictor class defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d37d92",
   "metadata": {},
   "source": [
    "## 4. Test Inference Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f365757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded random_forest v1.0.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize predictor\n",
    "predictor = NYCTaxiFarePredictor(str(production_model_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1f80405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Test data loaded: (2393196, 19)\n",
      "\n",
      "üìã Features available:\n",
      "['trip_distance', 'passenger_count', 'trip_duration_minutes', 'avg_speed_mph', 'pickup_hour', 'pickup_dayofweek', 'pickup_month', 'hour_sin', 'hour_cos', 'dow_sin', 'dow_cos', 'PULocationID', 'DOLocationID', 'VendorID', 'is_weekend', 'is_rush_hour', 'same_location', 'has_tolls']\n"
     ]
    }
   ],
   "source": [
    "# Load test data for sample inputs\n",
    "test_df = pd.read_parquet(PROCESSED_DATA_DIR / 'test.parquet')\n",
    "print(f\"üìä Test data loaded: {test_df.shape}\")\n",
    "print(f\"\\nüìã Features available:\")\n",
    "print(ALL_FEATURES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f25b967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Sample Input:\n",
      "   trip_distance: 0.7\n",
      "   passenger_count: 1.0\n",
      "   trip_duration_minutes: 6.7\n",
      "   avg_speed_mph: 6.26865671641791\n",
      "   pickup_hour: 10.0\n",
      "   pickup_dayofweek: 4.0\n",
      "   pickup_month: 3.0\n",
      "   hour_sin: 0.49999999999999994\n",
      "   hour_cos: -0.8660254037844387\n",
      "   dow_sin: -0.433883739117558\n",
      "   dow_cos: -0.9009688679024191\n",
      "   PULocationID: 170.0\n",
      "   DOLocationID: 162.0\n",
      "   VendorID: 1.0\n",
      "   is_weekend: 0.0\n",
      "   is_rush_hour: 0.0\n",
      "   same_location: 0.0\n",
      "   has_tolls: 0.0\n",
      "\n",
      "üéØ Prediction Results:\n",
      "   Predicted Fare: $7.56\n",
      "   Actual Fare:    $7.90\n",
      "   Difference:     $0.34\n"
     ]
    }
   ],
   "source": [
    "# Test single prediction\n",
    "sample_input = test_df[ALL_FEATURES].iloc[0].to_dict()\n",
    "actual_fare = test_df[TARGET].iloc[0]\n",
    "\n",
    "print(\"üìù Sample Input:\")\n",
    "for key, value in sample_input.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# Make prediction\n",
    "predicted_fare = predictor.predict(sample_input)\n",
    "\n",
    "print(f\"\\nüéØ Prediction Results:\")\n",
    "print(f\"   Predicted Fare: ${predicted_fare:.2f}\")\n",
    "print(f\"   Actual Fare:    ${actual_fare:.2f}\")\n",
    "print(f\"   Difference:     ${abs(predicted_fare - actual_fare):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18823efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Batch Prediction Test (n=100):\n",
      "   MAE: $1.1051\n",
      "   R¬≤:  0.9800\n",
      "\n",
      "‚úÖ Model inference working correctly!\n"
     ]
    }
   ],
   "source": [
    "# Test batch prediction\n",
    "batch_size = 100\n",
    "batch_data = test_df.head(batch_size)\n",
    "\n",
    "predictions = predictor.predict_batch(batch_data)\n",
    "actuals = batch_data[TARGET].values\n",
    "\n",
    "# Calculate metrics\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "mae = mean_absolute_error(actuals, predictions)\n",
    "r2 = r2_score(actuals, predictions)\n",
    "\n",
    "print(f\"üìä Batch Prediction Test (n={batch_size}):\")\n",
    "print(f\"   MAE: ${mae:.4f}\")\n",
    "print(f\"   R¬≤:  {r2:.4f}\")\n",
    "print(\"\\n‚úÖ Model inference working correctly!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7f5a10",
   "metadata": {},
   "source": [
    "## 5. Generate FastAPI Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5790590f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FastAPI script saved: ../src/api.py\n",
      "\n",
      "üìù To run the API:\n",
      "   cd mlops/src\n",
      "   uvicorn api:app --reload\n",
      "\n",
      "   Then visit: http://localhost:8000/docs\n"
     ]
    }
   ],
   "source": [
    "# Generate FastAPI script\n",
    "fastapi_code = '''\"\"\"FastAPI endpoint for NYC Taxi Fare Prediction.\"\"\"\n",
    "\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI(\n",
    "    title=\"NYC Taxi Fare Prediction API\",\n",
    "    description=\"Predict NYC taxi fares using ML model\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# Load model on startup\n",
    "MODEL_PATH = Path(__file__).parent.parent / \"models\" / \"production_model.joblib\"\n",
    "model_package = None\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "def load_model():\n",
    "    global model_package\n",
    "    model_package = joblib.load(MODEL_PATH)\n",
    "    print(f\"‚úÖ Model loaded: {model_package['model_name']} v{model_package['version']}\")\n",
    "\n",
    "# Request/Response models\n",
    "class PredictionRequest(BaseModel):\n",
    "    \"\"\"Input features for prediction.\"\"\"\n",
    "    trip_distance: float\n",
    "    pickup_hour: int\n",
    "    pickup_dayofweek: int\n",
    "    passenger_count: int = 1\n",
    "    # Add other features as needed based on your model\n",
    "    \n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"trip_distance\": 2.5,\n",
    "                \"pickup_hour\": 14,\n",
    "                \"pickup_dayofweek\": 2,\n",
    "                \"passenger_count\": 2\n",
    "            }\n",
    "        }\n",
    "\n",
    "class PredictionResponse(BaseModel):\n",
    "    \"\"\"Prediction result.\"\"\"\n",
    "    predicted_fare: float\n",
    "    model_name: str\n",
    "    model_version: str\n",
    "\n",
    "class HealthResponse(BaseModel):\n",
    "    \"\"\"Health check response.\"\"\"\n",
    "    status: str\n",
    "    model_loaded: bool\n",
    "    model_name: Optional[str]\n",
    "\n",
    "# Endpoints\n",
    "@app.get(\"/\")\n",
    "def root():\n",
    "    return {\"message\": \"NYC Taxi Fare Prediction API\", \"docs\": \"/docs\"}\n",
    "\n",
    "@app.get(\"/health\", response_model=HealthResponse)\n",
    "def health_check():\n",
    "    \"\"\"Check API health and model status.\"\"\"\n",
    "    return HealthResponse(\n",
    "        status=\"healthy\",\n",
    "        model_loaded=model_package is not None,\n",
    "        model_name=model_package['model_name'] if model_package else None\n",
    "    )\n",
    "\n",
    "@app.post(\"/predict\", response_model=PredictionResponse)\n",
    "def predict(request: PredictionRequest):\n",
    "    \"\"\"Make fare prediction.\"\"\"\n",
    "    if model_package is None:\n",
    "        raise HTTPException(status_code=503, detail=\"Model not loaded\")\n",
    "    \n",
    "    try:\n",
    "        # Convert request to DataFrame\n",
    "        data = pd.DataFrame([request.dict()])\n",
    "        \n",
    "        # Get features and model\n",
    "        features = model_package['features']\n",
    "        model = model_package['model']\n",
    "        \n",
    "        # Add missing features with default values\n",
    "        for feature in features:\n",
    "            if feature not in data.columns:\n",
    "                data[feature] = 0\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = model.predict(data[features])[0]\n",
    "        \n",
    "        return PredictionResponse(\n",
    "            predicted_fare=round(float(prediction), 2),\n",
    "            model_name=model_package['model_name'],\n",
    "            model_version=model_package['version']\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/model/info\")\n",
    "def model_info():\n",
    "    \"\"\"Get model information.\"\"\"\n",
    "    if model_package is None:\n",
    "        raise HTTPException(status_code=503, detail=\"Model not loaded\")\n",
    "    \n",
    "    return {\n",
    "        \"model_name\": model_package['model_name'],\n",
    "        \"model_type\": model_package['model_type'],\n",
    "        \"version\": model_package['version'],\n",
    "        \"features\": model_package['features'],\n",
    "        \"created_at\": model_package['created_at']\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "'''\n",
    "\n",
    "# Save FastAPI script\n",
    "api_path = SRC_DIR / 'api.py'\n",
    "with open(api_path, 'w') as f:\n",
    "    f.write(fastapi_code)\n",
    "\n",
    "print(f\"‚úÖ FastAPI script saved: {api_path}\")\n",
    "print(\"\\nüìù To run the API:\")\n",
    "print(\"   cd mlops/src\")\n",
    "print(\"   uvicorn api:app --reload\")\n",
    "print(\"\\n   Then visit: http://localhost:8000/docs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b252a38c",
   "metadata": {},
   "source": [
    "## 6. Generate Streamlit App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e70666f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Streamlit app saved: ../src/app.py\n",
      "\n",
      "üìù To run the app:\n",
      "   cd mlops/src\n",
      "   streamlit run app.py\n"
     ]
    }
   ],
   "source": [
    "# Generate Streamlit app\n",
    "streamlit_code = '''\"\"\"Streamlit App for NYC Taxi Fare Prediction.\"\"\"\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "# Page config\n",
    "st.set_page_config(\n",
    "    page_title=\"NYC Taxi Fare Predictor\",\n",
    "    page_icon=\"üöï\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "# Load model\n",
    "@st.cache_resource\n",
    "def load_model():\n",
    "    model_path = Path(__file__).parent.parent / \"models\" / \"production_model.joblib\"\n",
    "    return joblib.load(model_path)\n",
    "\n",
    "# Main app\n",
    "def main():\n",
    "    st.title(\"üöï NYC Taxi Fare Predictor\")\n",
    "    st.markdown(\"Predict taxi fare using Machine Learning\")\n",
    "    \n",
    "    # Load model\n",
    "    try:\n",
    "        model_package = load_model()\n",
    "        model = model_package['model']\n",
    "        features = model_package['features']\n",
    "        \n",
    "        st.sidebar.success(f\"‚úÖ Model: {model_package['model_name']}\")\n",
    "        st.sidebar.info(f\"Version: {model_package['version']}\")\n",
    "    except Exception as e:\n",
    "        st.error(f\"‚ùå Error loading model: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Sidebar inputs\n",
    "    st.sidebar.header(\"üéõÔ∏è Trip Details\")\n",
    "    \n",
    "    trip_distance = st.sidebar.slider(\n",
    "        \"Trip Distance (miles)\",\n",
    "        min_value=0.1,\n",
    "        max_value=30.0,\n",
    "        value=2.5,\n",
    "        step=0.1\n",
    "    )\n",
    "    \n",
    "    pickup_hour = st.sidebar.slider(\n",
    "        \"Pickup Hour\",\n",
    "        min_value=0,\n",
    "        max_value=23,\n",
    "        value=14\n",
    "    )\n",
    "    \n",
    "    pickup_dayofweek = st.sidebar.selectbox(\n",
    "        \"Day of Week\",\n",
    "        options=[0, 1, 2, 3, 4, 5, 6],\n",
    "        format_func=lambda x: [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"][x],\n",
    "        index=2\n",
    "    )\n",
    "    \n",
    "    passenger_count = st.sidebar.slider(\n",
    "        \"Passenger Count\",\n",
    "        min_value=1,\n",
    "        max_value=6,\n",
    "        value=1\n",
    "    )\n",
    "    \n",
    "    # Create input DataFrame\n",
    "    input_data = {\n",
    "        'trip_distance': trip_distance,\n",
    "        'pickup_hour': pickup_hour,\n",
    "        'pickup_dayofweek': pickup_dayofweek,\n",
    "        'passenger_count': passenger_count\n",
    "    }\n",
    "    \n",
    "    # Add missing features with default values\n",
    "    df = pd.DataFrame([input_data])\n",
    "    for feature in features:\n",
    "        if feature not in df.columns:\n",
    "            df[feature] = 0\n",
    "    \n",
    "    # Main content\n",
    "    col1, col2 = st.columns(2)\n",
    "    \n",
    "    with col1:\n",
    "        st.subheader(\"üìù Trip Information\")\n",
    "        st.write(f\"**Distance:** {trip_distance} miles\")\n",
    "        st.write(f\"**Time:** {pickup_hour}:00\")\n",
    "        st.write(f\"**Day:** {['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'][pickup_dayofweek]}\")\n",
    "        st.write(f\"**Passengers:** {passenger_count}\")\n",
    "    \n",
    "    with col2:\n",
    "        st.subheader(\"üí∞ Fare Prediction\")\n",
    "        \n",
    "        if st.button(\"üîÆ Predict Fare\", type=\"primary\"):\n",
    "            with st.spinner(\"Calculating...\"):\n",
    "                prediction = model.predict(df[features])[0]\n",
    "                \n",
    "                st.metric(\n",
    "                    label=\"Predicted Fare\",\n",
    "                    value=f\"${prediction:.2f}\"\n",
    "                )\n",
    "                \n",
    "                # Confidence message\n",
    "                if prediction < 10:\n",
    "                    st.info(\"üí° Short trip - typical for nearby destinations\")\n",
    "                elif prediction < 30:\n",
    "                    st.info(\"üí° Medium trip - common for cross-borough travel\")\n",
    "                else:\n",
    "                    st.info(\"üí° Long trip - possibly to/from airports\")\n",
    "    \n",
    "    # Footer\n",
    "    st.markdown(\"---\")\n",
    "    st.markdown(\"**üìä Model Info**\")\n",
    "    with st.expander(\"View Model Details\"):\n",
    "        st.json({\n",
    "            \"model_name\": model_package['model_name'],\n",
    "            \"model_type\": model_package['model_type'],\n",
    "            \"version\": model_package['version'],\n",
    "            \"num_features\": len(features),\n",
    "            \"created_at\": model_package['created_at']\n",
    "        })\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Save Streamlit app\n",
    "app_path = SRC_DIR / 'app.py'\n",
    "with open(app_path, 'w') as f:\n",
    "    f.write(streamlit_code)\n",
    "\n",
    "print(f\"‚úÖ Streamlit app saved: {app_path}\")\n",
    "print(\"\\nüìù To run the app:\")\n",
    "print(\"   cd mlops/src\")\n",
    "print(\"   streamlit run app.py\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b336fdf8",
   "metadata": {},
   "source": [
    "## 7. Generate Requirements File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ec2ef34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Requirements saved: ../requirements.txt\n"
     ]
    }
   ],
   "source": [
    "# Generate requirements.txt\n",
    "requirements = \"\"\"# Core ML\n",
    "pandas>=1.5.0\n",
    "numpy>=1.21.0\n",
    "scikit-learn>=1.0.0\n",
    "joblib>=1.1.0\n",
    "\n",
    "# API & Web\n",
    "fastapi>=0.100.0\n",
    "uvicorn>=0.22.0\n",
    "streamlit>=1.25.0\n",
    "pydantic>=2.0.0\n",
    "\n",
    "# Monitoring\n",
    "evidently>=0.7.0\n",
    "\n",
    "# Data\n",
    "pyarrow>=12.0.0\n",
    "\n",
    "# Visualization\n",
    "matplotlib>=3.5.0\n",
    "seaborn>=0.12.0\n",
    "\"\"\"\n",
    "\n",
    "req_path = Path('../requirements.txt')\n",
    "with open(req_path, 'w') as f:\n",
    "    f.write(requirements)\n",
    "\n",
    "print(f\"‚úÖ Requirements saved: {req_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eeabf0a",
   "metadata": {},
   "source": [
    "## 8. Test API Locally (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67f2e344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Simulating API Request...\n",
      "==================================================\n",
      "Request Body:\n",
      "{\n",
      "  \"trip_distance\": 2.5,\n",
      "  \"pickup_hour\": 14,\n",
      "  \"pickup_dayofweek\": 2,\n",
      "  \"passenger_count\": 2\n",
      "}\n",
      "\n",
      "Response Body:\n",
      "{\n",
      "  \"predicted_fare\": 21.16,\n",
      "  \"model_name\": \"random_forest\",\n",
      "  \"model_version\": \"1.0.0\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Simulate API request locally\n",
    "print(\"üìù Simulating API Request...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Sample request\n",
    "api_request = {\n",
    "    \"trip_distance\": 2.5,\n",
    "    \"pickup_hour\": 14,\n",
    "    \"pickup_dayofweek\": 2,\n",
    "    \"passenger_count\": 2\n",
    "}\n",
    "\n",
    "print(\"Request Body:\")\n",
    "print(json.dumps(api_request, indent=2))\n",
    "\n",
    "# Create DataFrame from request\n",
    "df_request = pd.DataFrame([api_request])\n",
    "for feature in ALL_FEATURES:\n",
    "    if feature not in df_request.columns:\n",
    "        df_request[feature] = 0\n",
    "\n",
    "# Make prediction\n",
    "prediction = model.predict(df_request[ALL_FEATURES])[0]\n",
    "\n",
    "# Response\n",
    "api_response = {\n",
    "    \"predicted_fare\": round(float(prediction), 2),\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"model_version\": \"1.0.0\"\n",
    "}\n",
    "\n",
    "print(\"\\nResponse Body:\")\n",
    "print(json.dumps(api_response, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0814c0ff",
   "metadata": {},
   "source": [
    "## 9. Deployment Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b886dc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "              üì¶ DEPLOYMENT SUMMARY\n",
      "======================================================================\n",
      "\n",
      "MODEL INFORMATION:\n",
      "   Model: random_forest\n",
      "   Version: 1.0.0\n",
      "   Features: 18\n",
      "   Target: fare_amount\n",
      "\n",
      "GENERATED FILES:\n",
      "   üìÅ models/\n",
      "      - production_model.joblib (model package)\n",
      "      - feature_config.json\n",
      "   \n",
      "   üìÅ src/\n",
      "      - api.py (FastAPI endpoint)\n",
      "      - app.py (Streamlit UI)\n",
      "   \n",
      "   üìÑ requirements.txt\n",
      "\n",
      "HOW TO RUN:\n",
      "\n",
      "   1. FastAPI (REST API):\n",
      "      cd mlops/src\n",
      "      pip install fastapi uvicorn\n",
      "      uvicorn api:app --reload\n",
      "      ‚Üí Open: http://localhost:8000/docs\n",
      "\n",
      "   2. Streamlit (Web UI):\n",
      "      cd mlops/src\n",
      "      pip install streamlit\n",
      "      streamlit run app.py\n",
      "      ‚Üí Open: http://localhost:8501\n",
      "\n",
      "API ENDPOINTS:\n",
      "   GET  /           ‚Üí Welcome message\n",
      "   GET  /health     ‚Üí Health check\n",
      "   POST /predict    ‚Üí Make prediction\n",
      "   GET  /model/info ‚Üí Model information\n",
      "\n",
      "======================================================================\n",
      "‚úÖ Deployment Preparation Complete!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"              üì¶ DEPLOYMENT SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "MODEL INFORMATION:\n",
    "   Model: {MODEL_NAME}\n",
    "   Version: 1.0.0\n",
    "   Features: {len(ALL_FEATURES)}\n",
    "   Target: {TARGET}\n",
    "\n",
    "GENERATED FILES:\n",
    "   üìÅ models/\n",
    "      - production_model.joblib (model package)\n",
    "      - feature_config.json\n",
    "   \n",
    "   üìÅ src/\n",
    "      - api.py (FastAPI endpoint)\n",
    "      - app.py (Streamlit UI)\n",
    "   \n",
    "   üìÑ requirements.txt\n",
    "\n",
    "HOW TO RUN:\n",
    "\n",
    "   1. FastAPI (REST API):\n",
    "      cd mlops/src\n",
    "      pip install fastapi uvicorn\n",
    "      uvicorn api:app --reload\n",
    "      ‚Üí Open: http://localhost:8000/docs\n",
    "\n",
    "   2. Streamlit (Web UI):\n",
    "      cd mlops/src\n",
    "      pip install streamlit\n",
    "      streamlit run app.py\n",
    "      ‚Üí Open: http://localhost:8501\n",
    "\n",
    "API ENDPOINTS:\n",
    "   GET  /           ‚Üí Welcome message\n",
    "   GET  /health     ‚Üí Health check\n",
    "   POST /predict    ‚Üí Make prediction\n",
    "   GET  /model/info ‚Üí Model information\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"‚úÖ Deployment Preparation Complete!\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d1698a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîó Untuk Dosen: Deployment Architecture\n",
    "\n",
    "### Production Architecture\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   Streamlit UI  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   FastAPI       ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   ML Model      ‚îÇ\n",
    "‚îÇ   (Frontend)    ‚îÇ     ‚îÇ   (Backend)     ‚îÇ     ‚îÇ   (Inference)   ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚îÇ                      ‚îÇ                       ‚îÇ\n",
    "         ‚ñº                      ‚ñº                       ‚ñº\n",
    "    User Input            Request/Response        Prediction\n",
    "```\n",
    "\n",
    "### Deployment Options\n",
    "\n",
    "| Platform | Streamlit | FastAPI | Cost |\n",
    "|----------|-----------|---------|------|\n",
    "| Local | ‚úÖ | ‚úÖ | Free |\n",
    "| Streamlit Cloud | ‚úÖ | ‚ùå | Free |\n",
    "| Heroku | ‚úÖ | ‚úÖ | Free tier |\n",
    "| AWS/GCP | ‚úÖ | ‚úÖ | Pay-as-you-go |\n",
    "\n",
    "### Next Steps\n",
    "1. Containerize with Docker\n",
    "2. Set up CI/CD pipeline\n",
    "3. Add authentication\n",
    "4. Set up monitoring dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "646de0db",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/processed/test.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load test data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m test_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/processed/test.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m train_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/processed/train.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124müìä TRAIN DATA HEAD:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parquet.py:267\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    265\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[1;32m    275\u001b[0m         path_or_handle,\n\u001b[1;32m    276\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    280\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parquet.py:140\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    130\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m     handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/processed/test.parquet'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load test data\n",
    "test_df = pd.read_parquet('data/processed/test.parquet')\n",
    "train_df = pd.read_parquet('data/processed/train.parquet')\n",
    "\n",
    "print('üìä TRAIN DATA HEAD:')\n",
    "print('='*80)\n",
    "print(train_df.head().to_string())\n",
    "print(f'\\nShape: {train_df.shape}')\n",
    "print(f'Columns: {list(train_df.columns)}')\n",
    "\n",
    "print('\\n\\nüìä TEST DATA HEAD:')\n",
    "print('='*80)\n",
    "print(test_df.head().to_string())\n",
    "print(f'\\nShape: {test_df.shape}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
